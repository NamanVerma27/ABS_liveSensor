{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7df2010a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- All libraries imported successfully. ---\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# --- Preprocessing ---\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler, PowerTransformer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# --- Imbalance Handling ---\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline # Special pipeline for samplers\n",
    "\n",
    "# --- Modeling ---\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import miceforest as mf\n",
    "\n",
    "# --- Evaluation ---\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "# --- Settings ---\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(\"--- All libraries imported successfully. ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c407c65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- `reduce_memory_usage` function defined. ---\n"
     ]
    }
   ],
   "source": [
    "# --- Function : Upgraded Memory Reducer from EDA---\n",
    "\n",
    "def reduce_memory_usage(df):\n",
    "    \"\"\"\n",
    "    Iterates through all columns of a DataFrame and modifies the data type\n",
    "    to reduce memory usage.\n",
    "    \n",
    "    - Checks if float columns contain only whole numbers.\n",
    "    - If they do, converts them to a memory-efficient nullable integer type\n",
    "      (e.g., pd.Int32Dtype()) to preserve NaNs.\n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    print(f'\\nInitial memory usage of the dataframe: {start_mem:.2f} MB')\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        # Skip the categorical 'class' column\n",
    "        if col == 'class':\n",
    "            continue\n",
    "            \n",
    "        # Downcast numerical columns\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "\n",
    "            # Check for integers\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            \n",
    "            # --- START: New Logic for Float Columns ---\n",
    "            else: # This is a float column\n",
    "                \n",
    "                # Check if all non-NaN values are whole numbers\n",
    "                if df[col].dropna().apply(lambda x: x.is_integer()).all():\n",
    "                    print(f\"Column '{col}' is float but contains only whole numbers. Converting to nullable int.\")\n",
    "                    \n",
    "                    # We can now use Pandas' Nullable Integer types\n",
    "                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                        df[col] = df[col].astype(pd.Int8Dtype())\n",
    "                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                        df[col] = df[col].astype(pd.Int16Dtype())\n",
    "                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                        df[col] = df[col].astype(pd.Int32Dtype())\n",
    "                    else:\n",
    "                        df[col] = df[col].astype(pd.Int64Dtype())\n",
    "                \n",
    "                else:\n",
    "                    # --- This is a \"real\" float column (has decimals) ---\n",
    "                    if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                        df[col] = df[col].astype(np.float32)\n",
    "                    else:\n",
    "                        df[col] = df[col].astype(np.float64)\n",
    "            # --- END: New Logic ---\n",
    "\n",
    "    end_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    print(f'Memory usage after optimization: {end_mem:.2f} MB')\n",
    "    print(f'Reduced by: {(100 * (start_mem - end_mem) / start_mem):.2f}%')\n",
    "    return df\n",
    "\n",
    "print(\"--- `reduce_memory_usage` function defined. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8918e922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- `total_cost` function defined. ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Function : Our Business Cost Function ---\n",
    "\n",
    "def total_cost(y_true, y_pred):\n",
    "    '''\n",
    "    Calculates the total cost based on the problem statement.\n",
    "    Cost 1 (False Positive) = 10\n",
    "    Cost 2 (False Negative) = 500\n",
    "    '''\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    try:\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        \n",
    "        # Handle cases where the model predicts only one class\n",
    "        if cm.shape == (1, 1):\n",
    "            if y_true.unique()[0] == 0: # Only negative class\n",
    "                tn = cm[0, 0]\n",
    "                fp, fn, tp = 0, 0, 0\n",
    "            else: # Only positive class\n",
    "                tp = cm[0, 0]\n",
    "                tn, fp, fn = 0, 0, 0\n",
    "        else:\n",
    "             # Standard 2x2 matrix\n",
    "            tn, fp, fn, tp = cm.ravel()\n",
    "            \n",
    "    except Exception as e:\n",
    "        # Fallback for complex edge cases\n",
    "        if len(np.unique(y_pred)) == 1:\n",
    "            if np.unique(y_pred)[0] == 0: # Predicted all negatives\n",
    "                tn = np.sum((y_true == 0))\n",
    "                fn = np.sum((y_true == 1))\n",
    "                fp, tp = 0, 0\n",
    "            else: # Predicted all positives\n",
    "                fp = np.sum((y_true == 0))\n",
    "                tp = np.sum((y_true == 1))\n",
    "                tn, fn = 0, 0\n",
    "        else:\n",
    "            print(f\"Error in confusion matrix calculation: {e}. Returning high cost.\")\n",
    "            return np.inf\n",
    "            \n",
    "    cost = (10 * fp) + (500 * fn)\n",
    "    return cost\n",
    "\n",
    "print(\"--- `total_cost` function defined. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00fc332b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Successfully loaded data from URL. Initial shape: (36188, 171) ---\n",
      "\n",
      "Initial memory usage of the dataframe: 48.73 MB\n",
      "Column 'ab_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ac_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ad_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ae_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'af_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ag_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ag_001' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ag_002' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ag_003' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ag_004' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ag_005' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ag_006' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ag_007' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ag_008' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ag_009' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ah_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ai_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'aj_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ak_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'al_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'am_0' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'an_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ao_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ap_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'aq_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ar_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'as_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'at_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'au_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'av_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ax_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ay_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ay_001' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ay_002' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ay_003' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ay_004' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ay_005' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ay_006' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ay_007' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ay_008' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ay_009' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'az_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'az_001' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'az_002' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'az_003' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'az_004' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'az_005' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'az_006' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'az_007' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'az_008' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'az_009' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ba_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ba_001' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ba_002' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ba_003' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ba_004' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ba_005' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ba_006' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ba_007' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ba_008' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ba_009' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'bb_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'bc_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'bd_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'be_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'bf_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'bg_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'bh_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'bi_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'bj_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'bk_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'bl_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'bm_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'bn_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'bo_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'bp_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'bq_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'br_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'bs_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'bu_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'bv_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'bx_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'by_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'bz_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ca_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cb_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cc_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cd_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ce_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cf_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cg_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ch_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cl_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cm_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cn_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cn_001' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cn_002' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cn_003' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cn_004' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cn_005' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cn_006' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cn_007' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cn_008' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cn_009' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'co_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cp_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cq_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cr_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cs_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cs_001' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cs_002' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cs_003' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cs_004' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cs_005' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cs_006' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cs_007' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cs_008' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cs_009' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ct_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cu_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cv_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cx_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cy_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cz_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'da_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'db_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'dc_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'dd_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'de_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'df_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'dg_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'dh_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'di_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'dj_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'dk_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'dl_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'dm_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'dn_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'do_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'dp_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'dq_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'dr_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ds_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'dt_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'du_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'dv_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'dx_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'dy_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'dz_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ea_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'eb_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ed_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ee_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ee_001' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ee_002' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ee_003' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ee_004' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ee_005' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ee_006' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ee_007' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ee_008' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ee_009' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ef_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'eg_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Memory usage after optimization: 30.58 MB\n",
      "Reduced by: 37.25%\n",
      "\n",
      "--- Encoding target variable 'class' ---\n",
      "Initial class counts:\n",
      "class\n",
      "neg    35188\n",
      "pos     1000\n",
      "Name: count, dtype: int64\n",
      "Encoded class counts:\n",
      "class\n",
      "0    35188\n",
      "1     1000\n",
      "Name: count, dtype: Int64\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Load Data ---\n",
    "file_path = \"https://raw.githubusercontent.com/avnyadav/sensor-fault-detection/main/aps_failure_training_set1.csv\"\n",
    "try:\n",
    "    df = pd.read_csv(file_path, na_values='na')\n",
    "    print(f\"--- Successfully loaded data from URL. Initial shape: {df.shape} ---\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    # Stop execution if data loading fails\n",
    "    raise\n",
    "\n",
    "# --- 2. Reduce Memory ---\n",
    "# This will use the upgraded function from Cell 1\n",
    "df = reduce_memory_usage(df)\n",
    "\n",
    "# --- 3. Encode Target Variable ---\n",
    "print(\"\\n--- Encoding target variable 'class' ---\")\n",
    "# Check if 'class' column exists\n",
    "if 'class' not in df.columns:\n",
    "    print(\"Error: 'class' column not found.\")\n",
    "else:\n",
    "    initial_class_counts = df['class'].value_counts()\n",
    "    print(f\"Initial class counts:\\n{initial_class_counts}\")\n",
    "    \n",
    "    # Map 'neg' to 0 and 'pos' to 1\n",
    "    df['class'] = df['class'].map({'neg': 0, 'pos': 1})\n",
    "    \n",
    "    # Verify mapping\n",
    "    if df['class'].isnull().any():\n",
    "        print(\"Warning: Found NaN values in 'class' column after mapping. Check for unexpected values.\")\n",
    "    else:\n",
    "        # Use a nullable int type here as well, just for consistency\n",
    "        df['class'] = df['class'].astype(pd.Int8Dtype())\n",
    "        print(f\"Encoded class counts:\\n{df['class'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b9a1bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting initial feature cleanup ---\n",
      "Initial number of features: 170\n",
      "Dropped 7 columns with > 70% missing values.\n",
      "Columns dropped: ['ab_000', 'bn_000', 'bo_000', 'bp_000', 'bq_000', 'br_000', 'cr_000']\n",
      "Dropped 1 columns with constant values (zero variance).\n",
      "Columns dropped: ['cd_000']\n",
      "\n",
      "--- Cleanup complete ---\n",
      "Final shape of X: (36188, 162)\n",
      "Final shape of y: (36188,)\n",
      "Total numeric features remaining: 162\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 4. Initial Feature Cleanup ---\n",
    "print(\"\\n--- Starting initial feature cleanup ---\")\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "if 'class' in df.columns:\n",
    "    X = df.drop('class', axis=1)\n",
    "    y = df['class']\n",
    "else:\n",
    "    raise ValueError(\"Cannot proceed without 'class' column for X/y split.\")\n",
    "\n",
    "print(f\"Initial number of features: {X.shape[1]}\")\n",
    "\n",
    "# --- 4a. Drop High-NaN Columns (> 70% missing) ---\n",
    "missing_percentage = (X.isnull().sum() / len(X)) * 100\n",
    "cols_to_drop_nan = missing_percentage[missing_percentage > 70].index\n",
    "\n",
    "if len(cols_to_drop_nan) > 0:\n",
    "    X = X.drop(columns=cols_to_drop_nan)\n",
    "    print(f\"Dropped {len(cols_to_drop_nan)} columns with > 70% missing values.\")\n",
    "    print(f\"Columns dropped: {list(cols_to_drop_nan)}\")\n",
    "else:\n",
    "    print(\"No columns had > 70% missing values.\")\n",
    "\n",
    "# --- 4b. Drop Constant Columns (Zero Variance) ---\n",
    "# We check for columns that have only 1 unique value (or 0 unique values if all NaN)\n",
    "unique_counts = X.nunique()\n",
    "cols_to_drop_constant = unique_counts[unique_counts <= 1].index\n",
    "\n",
    "if len(cols_to_drop_constant) > 0:\n",
    "    X = X.drop(columns=cols_to_drop_constant)\n",
    "    print(f\"Dropped {len(cols_to_drop_constant)} columns with constant values (zero variance).\")\n",
    "    print(f\"Columns dropped: {list(cols_to_drop_constant)}\")\n",
    "else:\n",
    "    print(\"No constant columns found.\")\n",
    "\n",
    "# --- 5. Final Check ---\n",
    "print(f\"\\n--- Cleanup complete ---\")\n",
    "print(f\"Final shape of X: {X.shape}\")\n",
    "print(f\"Final shape of y: {y.shape}\")\n",
    "\n",
    "# Store all remaining feature names for the ColumnTransformer\n",
    "# All remaining features are numeric\n",
    "numeric_features = list(X.columns)\n",
    "print(f\"Total numeric features remaining: {len(numeric_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7c997b",
   "metadata": {},
   "source": [
    "# Analysis of Cell 2 Achievements\n",
    "\n",
    "## What We Achieved\n",
    "\n",
    "* **Data Loaded:** We successfully loaded the full dataset from the URL, which started with **36,188 rows** and **171 columns** (170 features + 1 target).\n",
    "\n",
    "* **Memory Optimized:** The `reduce_memory_usage` function worked perfectly. The log message *Column like 'ab000, ae000 , ad000' is float but contains only whole numbers. Converting them to *nullable *int correct correctly.\n",
    "    * This stores \"integer-like\" data more efficiently while correctly preserving `NaN` values.\n",
    "    * Memory usage was reduced from 48.73 MB to 30.58 MB (a **37.25% reduction**).\n",
    "\n",
    "* **Target Encoded:** We've successfully converted the `class` column from 'neg'/'pos' to binary **0/1**.\n",
    "\n",
    "* **Initial Pruning:** We \"pruned\" our feature set `X` by removing 8 problematic columns:\n",
    "    * **High-NaN Columns ( $>70\\%$):** Dropped 7 columns (`['ab_000', 'bn_000', 'bo_000', 'bp_000', 'bq_000', 'br_000', 'cr_000']`). These are considered noise, not signal.\n",
    "    * **Constant Column (Zero Variance):** Dropped 1 column (`['cd_000']`). This column had no predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67db069d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Splitting data into Train and Test sets ---\n",
      "\n",
      "--- Verification of stratified split: ---\n",
      "X_train shape: (28950, 162)\n",
      "X_test shape: (7238, 162)\n",
      "\n",
      "--- Training Set Class Distribution ---\n",
      "class\n",
      "0    28150\n",
      "1      800\n",
      "Name: count, dtype: Int64\n",
      "Positive class percentage in train: 2.76%\n",
      "\n",
      "--- Test Set Class Distribution ---\n",
      "class\n",
      "0    7038\n",
      "1     200\n",
      "Name: count, dtype: Int64\n",
      "Positive class percentage in test: 2.76%\n"
     ]
    }
   ],
   "source": [
    "# --- 6. Create Train-Test Split (The \"Data Leakage Barrier\") ---\n",
    "print(\"\\n--- Splitting data into Train and Test sets ---\")\n",
    "\n",
    "# We use stratify=y to ensure the rare positive class is\n",
    "# represented in both train and test sets proportionally.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# --- 7. Verify the Split ---\n",
    "print(\"\\n--- Verification of stratified split: ---\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "\n",
    "print(\"\\n--- Training Set Class Distribution ---\")\n",
    "train_counts = y_train.value_counts()\n",
    "print(train_counts)\n",
    "print(f\"Positive class percentage in train: {(train_counts[1] / len(y_train) * 100):.2f}%\")\n",
    "\n",
    "print(\"\\n--- Test Set Class Distribution ---\")\n",
    "test_counts = y_test.value_counts()\n",
    "print(test_counts)\n",
    "print(f\"Positive class percentage in test: {(test_counts[1] / len(y_test) * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b1afb4",
   "metadata": {},
   "source": [
    "## Stratification:\n",
    "\n",
    "* Training Set: Has **800 'pos'** samples (80% of the total 1000).\n",
    "\n",
    "* Test Set: Has **200 'pos'** samples (20% of the total 1000).\n",
    "\n",
    "* The **2.76% positive class ratio** is identical in both sets, meaning our test set is a perfect representation of our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64299e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Defining Preprocessing & Modeling Pipelines ---\n",
      "Calculated 'scale_pos_weight' for XGBoost: 35.19\n",
      "\n",
      "--- Successfully defined 3 complete pipelines: ---\n",
      "1. pipeline_1 (Reference: Impute 0 -> Scale)\n",
      "2. pipeline_2 (EDA-Median: Impute Median -> Transform -> Scale)\n",
      "3. pipeline_3 (EDA-KNN: Transform -> Impute KNN -> Scale)\n",
      "\n",
      "All pipelines end with XGBClassifier using scale_pos_weight.\n"
     ]
    }
   ],
   "source": [
    "# --- 8. Define Phase 1 Pipelines (Pipelines 1, 2, 3) ---\n",
    "print(\"--- Defining Preprocessing & Modeling Pipelines ---\")\n",
    "\n",
    "# --- Define the Model we will use as our \"Judge\" ---\n",
    "# We use scale_pos_weight for imbalance, as it's often faster\n",
    "# and a good alternative to SMOTE. Let's calculate it.\n",
    "# scale_pos_weight = total_negatives / total_positives\n",
    "train_counts = y_train.value_counts()\n",
    "scale_pos_weight_value = train_counts[0] / train_counts[1]\n",
    "print(f\"Calculated 'scale_pos_weight' for XGBoost: {scale_pos_weight_value:.2f}\")\n",
    "\n",
    "# We will use this as our constant \"judge\" model\n",
    "# We set eval_metric to 'logloss' to avoid a common warning\n",
    "constant_model = XGBClassifier(\n",
    "    scale_pos_weight=scale_pos_weight_value,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "# === Pipeline 1: The \"Reference\" (Control Group) ===\n",
    "# Hypothesis: Filling NaNs with 0 is the best strategy.\n",
    "# Steps: 1. Impute(0) -> 2. Scale\n",
    "preprocessor_1_ref = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value=0)),\n",
    "            ('scaler', RobustScaler())])\n",
    "        , numeric_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# === Pipeline 2: \"EDA-Informed\" (Median + Skew Correction) ===\n",
    "# Hypothesis: Fixing skew is more important.\n",
    "# Steps: 1. Impute(Median) -> 2. Transform(Yeo-Johnson) -> 3. Scale\n",
    "preprocessor_2_eda = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('transformer', PowerTransformer(method='yeo-johnson')),\n",
    "            ('scaler', RobustScaler()) ])\n",
    "        , numeric_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# === Pipeline 3: \"EDA-Informed\" (KNN + Skew Correction) ===\n",
    "# Hypothesis: Smart imputer (KNN) after fixing skew is best.\n",
    "# Steps: 1. Transform(Yeo-Johnson) -> 2. Impute(KNN) -> 3. Scale\n",
    "preprocessor_3_knn = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline(steps=[\n",
    "            ('transformer', PowerTransformer(method='yeo-johnson')),\n",
    "            ('imputer', KNNImputer(n_neighbors=5)),\n",
    "            ('scaler', RobustScaler())\n",
    "        ]), numeric_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# --- Now, create the *full* pipelines (Prep + Model) ---\n",
    "# NOTE: We are using XGBoost's built-in `scale_pos_weight`\n",
    "# which is simpler and faster than adding a SMOTE step.\n",
    "# This is a common and robust way to handle imbalance.\n",
    "\n",
    "pipeline_1 = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_1_ref),\n",
    "    ('model', constant_model)\n",
    "])\n",
    "\n",
    "pipeline_2 = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_2_eda),\n",
    "    ('model', constant_model)\n",
    "])\n",
    "\n",
    "pipeline_3 = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_3_knn),\n",
    "    ('model', constant_model)\n",
    "])\n",
    "\n",
    "print(\"\\n--- Successfully defined 3 complete pipelines: ---\")\n",
    "print(\"1. pipeline_1 (Reference: Impute 0 -> Scale)\")\n",
    "print(\"2. pipeline_2 (EDA-Median: Impute Median -> Transform -> Scale)\")\n",
    "print(\"3. pipeline_3 (EDA-KNN: Transform -> Impute KNN -> Scale)\")\n",
    "print(\"\\nAll pipelines end with XGBClassifier using scale_pos_weight.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa73c1b4",
   "metadata": {},
   "source": [
    "# Analysis of Cell 9 Output\n",
    "\n",
    "## What We Achieved\n",
    "\n",
    "* **Pipelines Built:** We have successfully defined our first three experimental pipelines, encapsulating our competing hypotheses.\n",
    "    * **Hypothesis 1 (Reference):** `pipeline_1` is ready to test if `Impute(0) -> Scale` is the best path.\n",
    "    * **Hypothesis 2 (EDA-Median):** `pipeline_2` is ready to test if `Impute(Median) -> Transform -> Scale` is better.\n",
    "    * **Hypothesis 3 (EDA-KNN):** `pipeline_3` is ready to test if `Transform -> Impute(KNN) -> Scale` is the best.\n",
    "\n",
    "* **Imbalance Handled:** We've made a strategic choice to use XGBoost's built-in `scale_pos_weight` parameter (calculated at $35.19$). This is a powerful and computationally efficient way to force the model to pay $35.19\\text{x}$ more attention to the rare positive class, directly addressing our core problem (instead of using SMOTE).\n",
    "\n",
    "---\n",
    "\n",
    "## Key Findings\n",
    "\n",
    "* All our \"test tubes\" are labeled and sitting in the rack. We are now ready to run the actual experiment by fitting these pipelines to the training data and using them to predict on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a353840",
   "metadata": {},
   "source": [
    "# **Testing three pipelines for lowest cost** <sup> *~ (withouth smote)* </sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73d7c794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Pipeline 1 (Reference: Impute 0 -> Scale) ---\n",
      "Pipeline 1 finished in 9.89 seconds.\n",
      "COST for Pipeline 1: $19,820\n",
      "\n",
      "Classification Report for Pipeline 1:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Class 0 (neg)       0.99      1.00      0.99      7038\n",
      "Class 1 (pos)       0.83      0.81      0.82       200\n",
      "\n",
      "     accuracy                           0.99      7238\n",
      "    macro avg       0.91      0.90      0.91      7238\n",
      " weighted avg       0.99      0.99      0.99      7238\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "--- Training Pipeline 2 (EDA-Median: Impute Median -> Transform -> Scale) ---\n",
      "Pipeline 2 finished in 17.60 seconds.\n",
      "COST for Pipeline 2: $20,350\n",
      "\n",
      "Classification Report for Pipeline 2:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Class 0 (neg)       0.99      1.00      0.99      7038\n",
      "Class 1 (pos)       0.82      0.80      0.81       200\n",
      "\n",
      "     accuracy                           0.99      7238\n",
      "    macro avg       0.91      0.90      0.90      7238\n",
      " weighted avg       0.99      0.99      0.99      7238\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "--- Training Pipeline 3 (EDA-KNN: Transform -> Impute KNN -> Scale) ---\n",
      "NOTE: This pipeline will take the longest due to KNN Imputer.\n",
      "Pipeline 3 finished in 798.75 seconds.\n",
      "COST for Pipeline 3: $19,890\n",
      "\n",
      "Classification Report for Pipeline 3:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Class 0 (neg)       0.99      0.99      0.99      7038\n",
      "Class 1 (pos)       0.81      0.81      0.81       200\n",
      "\n",
      "     accuracy                           0.99      7238\n",
      "    macro avg       0.90      0.90      0.90      7238\n",
      " weighted avg       0.99      0.99      0.99      7238\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "--- Comparison of Pipeline 1, 2, and 3 ---\n",
      "{'Pipeline 1 (Reference)': np.int64(19820), 'Pipeline 2 (EDA-Median)': np.int64(20350), 'Pipeline 3 (EDA-KNN)': np.int64(19890)}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# We will store the results of our experiments in this dictionary\n",
    "phase_1_results = {}\n",
    "\n",
    "# --- 1. Fit and Evaluate Pipeline 1 (Reference) ---\n",
    "print(\"--- Training Pipeline 1 (Reference: Impute 0 -> Scale) ---\")\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Train the pipeline\n",
    "    pipeline_1.fit(X_train, y_train)\n",
    "    \n",
    "    # Get predictions on the test set\n",
    "    y_pred_1 = pipeline_1.predict(X_test)\n",
    "    \n",
    "    # Calculate the total cost\n",
    "    cost_1 = total_cost(y_test, y_pred_1)\n",
    "    \n",
    "    # Store the result\n",
    "    phase_1_results['Pipeline 1 (Reference)'] = cost_1\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Pipeline 1 finished in {end_time - start_time:.2f} seconds.\")\n",
    "    print(f\"COST for Pipeline 1: ${cost_1:,.0f}\")\n",
    "    \n",
    "    print(\"\\nClassification Report for Pipeline 1:\")\n",
    "    # We must convert y_test (nullable int) to a standard int for the report\n",
    "    print(classification_report(y_test.astype(int), y_pred_1, target_names=['Class 0 (neg)', 'Class 1 (pos)']))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error training Pipeline 1: {e}\")\n",
    "    phase_1_results['Pipeline 1 (Reference)'] = float('inf') # Assign high cost on failure\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# --- 2. Fit and Evaluate Pipeline 2 (EDA-Median) ---\n",
    "print(\"--- Training Pipeline 2 (EDA-Median: Impute Median -> Transform -> Scale) ---\")\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Train the pipeline\n",
    "    pipeline_2.fit(X_train, y_train)\n",
    "    \n",
    "    # Get predictions on the test set\n",
    "    y_pred_2 = pipeline_2.predict(X_test)\n",
    "    \n",
    "    # Calculate the total cost\n",
    "    cost_2 = total_cost(y_test, y_pred_2)\n",
    "    \n",
    "    # Store the result\n",
    "    phase_1_results['Pipeline 2 (EDA-Median)'] = cost_2\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Pipeline 2 finished in {end_time - start_time:.2f} seconds.\")\n",
    "    print(f\"COST for Pipeline 2: ${cost_2:,.0f}\")\n",
    "    \n",
    "    print(\"\\nClassification Report for Pipeline 2:\")\n",
    "    print(classification_report(y_test.astype(int), y_pred_2, target_names=['Class 0 (neg)', 'Class 1 (pos)']))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error training Pipeline 2: {e}\")\n",
    "    phase_1_results['Pipeline 2 (EDA-Median)'] = float('inf')\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# --- 3. Fit and Evaluate Pipeline 3 (EDA-KNN) ---\n",
    "print(\"--- Training Pipeline 3 (EDA-KNN: Transform -> Impute KNN -> Scale) ---\")\n",
    "print(\"NOTE: This pipeline will take the longest due to KNN Imputer.\")\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Train the pipeline\n",
    "    pipeline_3.fit(X_train, y_train)\n",
    "    \n",
    "    # Get predictions on the test set\n",
    "    y_pred_3 = pipeline_3.predict(X_test)\n",
    "    \n",
    "    # Calculate the total cost\n",
    "    cost_3 = total_cost(y_test, y_pred_3)\n",
    "    \n",
    "    # Store the result\n",
    "    phase_1_results['Pipeline 3 (EDA-KNN)'] = cost_3\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Pipeline 3 finished in {end_time - start_time:.2f} seconds.\")\n",
    "    print(f\"COST for Pipeline 3: ${cost_3:,.0f}\")\n",
    "    \n",
    "    print(\"\\nClassification Report for Pipeline 3:\")\n",
    "    print(classification_report(y_test.astype(int), y_pred_3, target_names=['Class 0 (neg)', 'Class 1 (pos)']))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error training Pipeline 3: {e}\")\n",
    "    phase_1_results['Pipeline 3 (EDA-KNN)'] = float('inf')\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "print(\"--- Comparison of Pipeline 1, 2, and 3 ---\")\n",
    "print(phase_1_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56472611",
   "metadata": {},
   "source": [
    "# **Testing three pipelines for lowest cost** <sup> *~ (with smote)* </sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce248664",
   "metadata": {},
   "source": [
    "## PIPELINE - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3651ed41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Defining a new pipeline to test SMOTE ---\n",
      "Pipeline: Impute 0 -> Scale -> SMOTE -> XGBoost (no weights)\n",
      "New pipeline 'pipeline_1_smote' is defined.\n",
      "\n",
      "--- Training Pipeline 1 with SMOTE ---\n",
      "SMOTE pipeline finished in 10.84 seconds.\n",
      "COST for Pipeline 1 (SMOTE): $18,940\n",
      "\n",
      "Classification Report for Pipeline 1 (SMOTE):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Class 0 (neg)       0.99      0.99      0.99      7038\n",
      "Class 1 (pos)       0.79      0.81      0.80       200\n",
      "\n",
      "     accuracy                           0.99      7238\n",
      "    macro avg       0.89      0.90      0.90      7238\n",
      " weighted avg       0.99      0.99      0.99      7238\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "--- Updated Comparison (including SMOTE) ---\n",
      "\n",
      "--- Phase 1 Current Ranking (Lowest Cost is Best) ---\n",
      "1. Pipeline 1 (SMOTE): $18,940\n",
      "\n",
      "CURRENT WINNER for Phase 1: Pipeline 1 (SMOTE)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# We will store the results of our experiments in this dictionary\n",
    "phase_1_results = {}\n",
    "\n",
    "# --- 5.1. Define Pipeline 1 with SMOTE (Corrected) ---\n",
    "print(\"--- Defining a new pipeline to test SMOTE ---\")\n",
    "print(\"Pipeline: Impute 0 -> Scale -> SMOTE -> XGBoost (no weights)\")\n",
    "\n",
    "# 1. We use the preprocessor from our current winner (pipeline_1)\n",
    "if 'preprocessor_1_ref' not in locals():\n",
    "     print(\"Error: 'preprocessor_1_ref' not found. Please re-run Cell 4.\")\n",
    "     raise NameError(\"Missing 'preprocessor_1_ref'\")\n",
    "\n",
    "# 2. Define a new model, *without* scale_pos_weight\n",
    "model_for_smote = XGBClassifier(\n",
    "    random_state=42,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "# 3. Create the new Imbalanced-Learn (ImbPipeline)\n",
    "#    --- CORRECTION: Removed 'n_jobs=-1' from SMOTE() ---\n",
    "pipeline_1_smote = ImbPipeline(steps=[\n",
    "    ('preprocessor', preprocessor_1_ref),\n",
    "    ('smote', SMOTE(random_state=42)), # <-- n_jobs removed here\n",
    "    ('model', model_for_smote)\n",
    "])\n",
    "\n",
    "print(\"New pipeline 'pipeline_1_smote' is defined.\")\n",
    "\n",
    "# --- 5.2. Fit and Evaluate the SMOTE Pipeline ---\n",
    "print(\"\\n--- Training Pipeline 1 with SMOTE ---\")\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Train the pipeline\n",
    "    # --- SAFETY: Convert y_train to standard int for SMOTE ---\n",
    "    pipeline_1_smote.fit(X_train, y_train.astype(int))\n",
    "    \n",
    "    # Get predictions on the test set\n",
    "    y_pred_smote = pipeline_1_smote.predict(X_test)\n",
    "    \n",
    "    # Calculate the total cost\n",
    "    cost_smote = total_cost(y_test, y_pred_smote)\n",
    "    \n",
    "    # Store the result\n",
    "    phase_1_results['Pipeline 1 (SMOTE)'] = cost_smote\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"SMOTE pipeline finished in {end_time - start_time:.2f} seconds.\")\n",
    "    print(f\"COST for Pipeline 1 (SMOTE): ${cost_smote:,.0f}\")\n",
    "    \n",
    "    print(\"\\nClassification Report for Pipeline 1 (SMOTE):\")\n",
    "    print(classification_report(y_test.astype(int), y_pred_smote, target_names=['Class 0 (neg)', 'Class 1 (pos)']))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error training SMOTE Pipeline: {e}\")\n",
    "    phase_1_results['Pipeline 1 (SMOTE)'] = float('inf')\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "print(\"--- Updated Comparison (including SMOTE) ---\")\n",
    "\n",
    "# --- Final Verdict for Phase 1 ---\n",
    "# Sort the results by cost\n",
    "sorted_results = sorted(phase_1_results.items(), key=lambda item: item[1])\n",
    "\n",
    "print(\"\\n--- Phase 1 Current Ranking (Lowest Cost is Best) ---\")\n",
    "for i, (pipeline_name, cost) in enumerate(sorted_results):\n",
    "    print(f\"{i+1}. {pipeline_name}: ${cost:,.0f}\")\n",
    "\n",
    "# Store the winner for Phase 2\n",
    "winning_pipeline_name = sorted_results[0][0]\n",
    "print(f\"\\nCURRENT WINNER for Phase 1: {winning_pipeline_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e54649",
   "metadata": {},
   "source": [
    "## PIPELINE - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f952bf69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Defining and Testing Pipeline 2 (EDA-Median) with SMOTE ---\n",
      "Pipeline: Impute Median -> Transform -> Scale -> SMOTE -> XGBoost (no weights)\n",
      "New pipeline 'pipeline_2_smote' is defined.\n",
      "Pipeline 2 (SMOTE) finished in 16.52 seconds.\n",
      "COST for Pipeline 2 (SMOTE): $20,330\n",
      "\n",
      "Classification Report for Pipeline 2 (SMOTE):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Class 0 (neg)       0.99      1.00      0.99      7038\n",
      "Class 1 (pos)       0.83      0.80      0.81       200\n",
      "\n",
      "     accuracy                           0.99      7238\n",
      "    macro avg       0.91      0.90      0.90      7238\n",
      " weighted avg       0.99      0.99      0.99      7238\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "--- Updated Comparison ---\n",
      "\n",
      "--- Phase 1 Current Ranking (Lowest Cost is Best) ---\n",
      "1. Pipeline 3 (EDA-KNN-SMOTE): $18,870\n",
      "2. Pipeline 1 (SMOTE): $18,940\n",
      "3. Pipeline 1 (Reference): $19,820\n",
      "4. Pipeline 3 (EDA-KNN): $19,890\n",
      "5. Pipeline 2 (EDA-Median-SMOTE): $20,330\n",
      "6. Pipeline 2 (EDA-Median): $20,350\n",
      "\n",
      "CURRENT WINNER for Phase 1: Pipeline 3 (EDA-KNN-SMOTE)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# --- 5.2.1. Define and Run Pipeline 2 with SMOTE ---\n",
    "print(\"--- Defining and Testing Pipeline 2 (EDA-Median) with SMOTE ---\")\n",
    "print(\"Pipeline: Impute Median -> Transform -> Scale -> SMOTE -> XGBoost (no weights)\")\n",
    "\n",
    "# 1. We use the preprocessor from Pipeline 2\n",
    "#    (This variable 'preprocessor_2_eda' must be in memory from Cell 4)\n",
    "if 'preprocessor_2_eda' not in locals():\n",
    "     print(\"Error: 'preprocessor_2_eda' not found. Please re-run Cell 4.\")\n",
    "     raise NameError(\"Missing 'preprocessor_2_eda'\")\n",
    "\n",
    "# 2. We use the same 'model_for_smote' from Cell 5.1\n",
    "if 'model_for_smote' not in locals():\n",
    "     # In case Cell 5.1 was modified, let's redefine it to be safe\n",
    "     model_for_smote = XGBClassifier(\n",
    "         random_state=42,\n",
    "         eval_metric='logloss'\n",
    "     )\n",
    "     print(\"Redefined 'model_for_smote'.\")\n",
    "\n",
    "# 3. Create the new ImbPipeline\n",
    "pipeline_2_smote = ImbPipeline(steps=[\n",
    "    ('preprocessor', preprocessor_2_eda),\n",
    "    ('smote', SMOTE(random_state=42)), # <-- Our winning imbalance handler\n",
    "    ('model', model_for_smote)\n",
    "])\n",
    "\n",
    "print(\"New pipeline 'pipeline_2_smote' is defined.\")\n",
    "\n",
    "# --- Fit and Evaluate Pipeline 2 (SMOTE) ---\n",
    "start_time = time.time()\n",
    "try:\n",
    "    pipeline_2_smote.fit(X_train, y_train.astype(int))\n",
    "    y_pred_2_smote = pipeline_2_smote.predict(X_test)\n",
    "    cost_2_smote = total_cost(y_test, y_pred_2_smote)\n",
    "    \n",
    "    phase_1_results['Pipeline 2 (EDA-Median-SMOTE)'] = cost_2_smote\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Pipeline 2 (SMOTE) finished in {end_time - start_time:.2f} seconds.\")\n",
    "    print(f\"COST for Pipeline 2 (SMOTE): ${cost_2_smote:,.0f}\")\n",
    "    \n",
    "    print(\"\\nClassification Report for Pipeline 2 (SMOTE):\")\n",
    "    print(classification_report(y_test.astype(int), y_pred_2_smote, target_names=['Class 0 (neg)', 'Class 1 (pos)']))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error training Pipeline 2 (SMOTE): {e}\")\n",
    "    phase_1_results['Pipeline 2 (EDA-Median-SMOTE)'] = float('inf')\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "print(\"--- Updated Comparison ---\")\n",
    "\n",
    "# --- Final Verdict for Phase 1 ---\n",
    "# Sort the results by cost\n",
    "sorted_results = sorted(phase_1_results.items(), key=lambda item: item[1])\n",
    "\n",
    "print(\"\\n--- Phase 1 Current Ranking (Lowest Cost is Best) ---\")\n",
    "for i, (pipeline_name, cost) in enumerate(sorted_results):\n",
    "    print(f\"{i+1}. {pipeline_name}: ${cost:,.0f}\")\n",
    "\n",
    "# Store the winner for Phase 2\n",
    "winning_pipeline_name = sorted_results[0][0]\n",
    "print(f\"\\nCURRENT WINNER for Phase 1: {winning_pipeline_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f23ecf",
   "metadata": {},
   "source": [
    "## PIPELINE - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2af865d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Defining and Testing Pipeline 3 (EDA-KNN) with SMOTE ---\n",
      "Pipeline: Transform -> Impute KNN -> Scale -> SMOTE -> XGBoost (no weights)\n",
      "WARNING: THIS WILL TAKE A VERY LONG TIME TO RUN.\n",
      "New pipeline 'pipeline_3_smote' is defined.\n",
      "Pipeline 3 (SMOTE) finished in 821.83 seconds.\n",
      "COST for Pipeline 3 (SMOTE): $18,870\n",
      "\n",
      "Classification Report for Pipeline 3 (SMOTE):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Class 0 (neg)       0.99      0.99      0.99      7038\n",
      "Class 1 (pos)       0.81      0.81      0.81       200\n",
      "\n",
      "     accuracy                           0.99      7238\n",
      "    macro avg       0.90      0.90      0.90      7238\n",
      " weighted avg       0.99      0.99      0.99      7238\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "--- Updated Comparison ---\n",
      "\n",
      "--- Phase 1 Current Ranking (Lowest Cost is Best) ---\n",
      "1. Pipeline 3 (EDA-KNN-SMOTE): $18,870\n",
      "2. Pipeline 1 (SMOTE): $18,940\n",
      "5. Pipeline 2 (EDA-Median-SMOTE): $20,330\n",
      "\n",
      "CURRENT WINNER for Phase 1: Pipeline 3 (EDA-KNN-SMOTE)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# --- 5.3.1. Define and Run Pipeline 3 with SMOTE ---\n",
    "print(\"--- Defining and Testing Pipeline 3 (EDA-KNN) with SMOTE ---\")\n",
    "print(\"Pipeline: Transform -> Impute KNN -> Scale -> SMOTE -> XGBoost (no weights)\")\n",
    "print(\"WARNING: THIS WILL TAKE A VERY LONG TIME TO RUN.\")\n",
    "\n",
    "# 1. We use the preprocessor from Pipeline 3\n",
    "#    (This variable 'preprocessor_3_knn' must be in memory from Cell 4)\n",
    "if 'preprocessor_3_knn' not in locals():\n",
    "     print(\"Error: 'preprocessor_3_knn' not found. Please re-run Cell 4.\")\n",
    "     raise NameError(\"Missing 'preprocessor_3_knn'\")\n",
    "\n",
    "# 2. We use the same 'model_for_smote'\n",
    "if 'model_for_smote' not in locals():\n",
    "     model_for_smote = XGBClassifier(\n",
    "         random_state=42,\n",
    "         eval_metric='logloss'\n",
    "     )\n",
    "     print(\"Redefined 'model_for_smote'.\")\n",
    "\n",
    "# 3. Create the new ImbPipeline\n",
    "pipeline_3_smote = ImbPipeline(steps=[\n",
    "    ('preprocessor', preprocessor_3_knn),\n",
    "    ('smote', SMOTE(random_state=42)), # <-- Our winning imbalance handler\n",
    "    ('model', model_for_smote)\n",
    "])\n",
    "\n",
    "print(\"New pipeline 'pipeline_3_smote' is defined.\")\n",
    "\n",
    "# --- Fit and Evaluate Pipeline 3 (SMOTE) ---\n",
    "start_time = time.time()\n",
    "try:\n",
    "    pipeline_3_smote.fit(X_train, y_train.astype(int))\n",
    "    y_pred_3_smote = pipeline_3_smote.predict(X_test)\n",
    "    cost_3_smote = total_cost(y_test, y_pred_3_smote)\n",
    "    \n",
    "    # We use a new key to distinguish from the non-SMOTE version\n",
    "    phase_1_results['Pipeline 3 (EDA-KNN-SMOTE)'] = cost_3_smote\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Pipeline 3 (SMOTE) finished in {end_time - start_time:.2f} seconds.\")\n",
    "    print(f\"COST for Pipeline 3 (SMOTE): ${cost_3_smote:,.0f}\")\n",
    "    \n",
    "    print(\"\\nClassification Report for Pipeline 3 (SMOTE):\")\n",
    "    print(classification_report(y_test.astype(int), y_pred_3_smote, target_names=['Class 0 (neg)', 'Class 1 (pos)']))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error training Pipeline 3 (SMOTE): {e}\")\n",
    "    phase_1_results['Pipeline 3 (EDA-KNN-SMOTE)'] = float('inf')\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "print(\"--- Updated Comparison ---\")\n",
    "\n",
    "# --- Final Verdict for Phase 1 ---\n",
    "# Sort the results by cost\n",
    "sorted_results = sorted(phase_1_results.items(), key=lambda item: item[1])\n",
    "\n",
    "print(\"\\n--- Phase 1 Current Ranking (Lowest Cost is Best) ---\")\n",
    "for i, (pipeline_name, cost) in enumerate(sorted_results):\n",
    "    # We can remove the old, irrelevant (non-SMOTE) results for a cleaner list\n",
    "    if 'SMOTE' in pipeline_name:\n",
    "        print(f\"{i+1}. {pipeline_name}: ${cost:,.0f}\")\n",
    "\n",
    "# Find the new winner\n",
    "winning_pipeline_name = sorted_results[0][0]\n",
    "print(f\"\\nCURRENT WINNER for Phase 1: {winning_pipeline_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf25f17f",
   "metadata": {},
   "source": [
    "# Phase 1 Experimental Summary & Key Learnings\n",
    "\n",
    "We have now completed a rigorous series of experiments to find the best-performing preprocessing pipeline. We've tested 3 main preprocessing hypotheses and 2 different imbalance-handling strategies.\n",
    "\n",
    "Our \"apples-to-apples\" showdown in Cell 5 (where all pipelines were fairly paired with SMOTE) has produced a clear set of results.\n",
    "\n",
    "## Final Experimental Ranking (SMOTE-based)\n",
    "\n",
    "This table shows the final, \"apples-to-apples\" comparison of our three primary preprocessing strategies.\n",
    "\n",
    "| Rank | Pipeline | Final Cost | Training Time |\n",
    "|---:|:---|---:|---:|\n",
    "| **1.** | `P3 (EDA-KNN-SMOTE)` | **$18,870** | `821.83 sec` |\n",
    "| **2.** | `P1 (Reference + SMOTE)` | `$18,940` | `11.18 sec` |\n",
    "| **3.** | `P2 (EDA-Median-SMOTE)` | `$20,330` | `18.62 sec` |\n",
    "\n",
    "---\n",
    "\n",
    "## Key Strategic Insights (Our Story)\n",
    "\n",
    "This experiment told a very clear story.\n",
    "\n",
    "### 1. The Imbalance Handler is the Most Important Choice\n",
    "\n",
    "Our single most critical finding was that **SMOTE** is a vastly superior strategy to `scale_pos_weight` for this problem.\n",
    "\n",
    "* `P1 (scale_pos_weight)` Cost: **$19,820**\n",
    "* `P1 (SMOTE)` Cost: **$18,940**\n",
    "\n",
    "**Analysis:** By creating new synthetic data, SMOTE allowed the model to save **2 additional trucks** (fewer False Negatives). This saved us $1,000 in breakdown costs, for a minor penalty of $120 in false alarms. This **$880 net saving** proves that SMOTE is the correct strategy.\n",
    "\n",
    "### 2. Our EDA-Based \"Skew\" Hypothesis was Invalidated\n",
    "\n",
    "Our hypothesis from the EDA (in Pipeline 2) was that fixing the extreme skewness of the data with a **PowerTransformer** would be a key to success. The data proves this was incorrect.\n",
    "\n",
    "* `P1 (SMOTE)` Cost: **$18,940**\n",
    "* `P2 (SMOTE)` Cost: **$20,330**\n",
    "\n",
    "**Analysis:** The `PowerTransformer` step actively hurt performance. It caused the model to miss **3 extra trucks** (40 False Negatives vs. 37), adding $1,500 to our total cost. We've learned that the simple `Impute(0)` strategy is more robust.\n",
    "\n",
    "### 3. We Found a Classic \"Cost vs. Time\" Trade-off\n",
    "\n",
    "This is the final and most important business conclusion. We have two \"winners\":\n",
    "\n",
    "* **The Statistical Winner:** `Pipeline 3 (PowerTransformer-KNN-RobusrScaler-SMOTE)` has the lowest cost at **$18,870**.\n",
    "* **The Practical Winner:** `Pipeline 1 (Impute(0)-RobustScaler-SMOTE)` is only **$70 more expensive** (a 0.37% difference) but is **7,250% faster** (11 seconds vs. 13.5 minutes).\n",
    "\n",
    "**Analysis:** The `P3` pipeline (with KNN) did not save any more trucks. It had the exact same 37 False Negatives as `P1`. Its entire $70 victory came from being slightly more precise (fewer false alarms). In any real-world production environment, a $70 saving is not worth an 800-second increase in training time.\n",
    "\n",
    "---\n",
    "\n",
    "## Final Conclusion & Next Steps\n",
    "\n",
    "Our experiments have been incredibly successful. We have:\n",
    "\n",
    "* **Proven `SMOTE`** is the correct imbalance strategy.\n",
    "* **Invalidated** the `PowerTransformer` (skew-correction) hypothesis.\n",
    "* **Established** a clear **Practical Champion**:\n",
    "    * **Pipeline 1 (`Impute(0) -> RobustScaler -> SMOTE`)** with a cost of **$18,940**.\n",
    "\n",
    "This is our new champion to beat. The final remaining experiment, **Phase 2**, must now performed to test this pipeline for different models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e7525f",
   "metadata": {},
   "source": [
    "# Phase 2: The \"Model Bake-Off\"\n",
    "\n",
    "Now that we have **locked in our preprocessor**, we can move to Phase 2: finding the best model.\n",
    "\n",
    "Our goal is to see if any other model can beat the **$18,940 cost** achieved by `XGBClassifier`, using the exact same preprocessed data.\n",
    "\n",
    "## The Plan\n",
    "\n",
    "1.  **Prepare Data (Once):** We will create one master, pre-processed, and resampled training set.\n",
    "    * Apply the winning preprocessor (`Impute(0) -> RobustScaler`) to `X_train`.\n",
    "    * Apply `SMOTE` to the result.\n",
    "2.  **Train Models:** We will train our 4 \"Contender\" models on this identical dataset.\n",
    "3.  **Judge:** We will use the trained models to predict on the processed `X_test` and find the one with the lowest `total_cost`.\n",
    "\n",
    "## The Contenders\n",
    "\n",
    "* `XGBClassifier` (Our Current Champion, Baseline: **$18,940**)\n",
    "* `CatBoostClassifier` (A powerful competitor, often great with noisy data)\n",
    "* `RandomForestClassifier` (A robust, non-boosting alternative)\n",
    "* `LogisticRegression` (A simple baseline to see how \"clean\" our data is)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89ba4555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Preparing data using our 'Practical Winner' pipeline ---\n",
      "Strategy: Impute(0) -> RobustScaler -> SMOTE\n",
      "\n",
      "1. Fitting winning preprocessor on X_train...\n",
      "2. Transforming X_train and X_test...\n",
      "   Shape of X_train_processed: (28950, 162)\n",
      "   Shape of X_test_processed: (7238, 162)\n",
      "\n",
      "3. Applying SMOTE to the processed training data...\n",
      "   SMOTE resampling complete.\n",
      "\n",
      "--- Data is ready for Phase 2 ---\n",
      "Final Resampled Training Data (X): (56300, 162)\n",
      "Final Resampled Training Data (y): (56300,)\n",
      "Final Test Data (X): (7238, 162)\n",
      "Final Test Data (y): (7238,)\n",
      "\n",
      "Verification of resampled 'y' data:\n",
      "class\n",
      "0    28150\n",
      "1    28150\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# --- 7. Prepare Data for Phase 2 \"Model Bake-Off\" ---\n",
    "print(\"--- Preparing data using our 'Practical Winner' pipeline ---\")\n",
    "print(\"Strategy: Impute(0) -> RobustScaler -> SMOTE\")\n",
    "\n",
    "# --- 7.1. Process the Data ---\n",
    "\n",
    "# 1. Select our winning preprocessor\n",
    "#    (This variable 'preprocessor_1_ref' must be in memory from Cell 4)\n",
    "if 'preprocessor_1_ref' not in locals():\n",
    "     print(\"Error: 'preprocessor_1_ref' not found. Please re-run Cell 4.\")\n",
    "     raise NameError(\"Missing 'preprocessor_1_ref'\")\n",
    "\n",
    "print(\"\\n1. Fitting winning preprocessor on X_train...\")\n",
    "# Fit the preprocessor on X_train\n",
    "preprocessor_1_ref.fit(X_train)\n",
    "\n",
    "print(\"2. Transforming X_train and X_test...\")\n",
    "# Transform both X_train and X_test\n",
    "# We get back numpy arrays, which is fine\n",
    "X_train_processed = preprocessor_1_ref.transform(X_train)\n",
    "X_test_processed = preprocessor_1_ref.transform(X_test)\n",
    "\n",
    "print(f\"   Shape of X_train_processed: {X_train_processed.shape}\")\n",
    "print(f\"   Shape of X_test_processed: {X_test_processed.shape}\")\n",
    "\n",
    "# --- 7.2. Apply SMOTE ---\n",
    "print(\"\\n3. Applying SMOTE to the processed training data...\")\n",
    "# Initialize SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Fit and resample *only* the training data\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(\n",
    "    X_train_processed, \n",
    "    y_train.astype(int)\n",
    ")\n",
    "\n",
    "print(\"   SMOTE resampling complete.\")\n",
    "\n",
    "# --- 7.3. Final Verification ---\n",
    "print(\"\\n--- Data is ready for Phase 2 ---\")\n",
    "print(f\"Final Resampled Training Data (X): {X_train_resampled.shape}\")\n",
    "print(f\"Final Resampled Training Data (y): {y_train_resampled.shape}\")\n",
    "print(f\"Final Test Data (X): {X_test_processed.shape}\")\n",
    "print(f\"Final Test Data (y): {y_test.shape}\")\n",
    "\n",
    "print(\"\\nVerification of resampled 'y' data:\")\n",
    "# This should show 28150 'pos' and 28150 'neg'\n",
    "print(pd.Series(y_train_resampled).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bdf19cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Phase 2: The Model Bake-Off ---\n",
      "We are testing 4 models on our single, processed dataset.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "--- Training Model: Logistic Regression ---\n",
      "Logistic Regression finished in 44.55 seconds.\n",
      "COST for Logistic Regression: $51,520\n",
      "\n",
      "Classification Report for Logistic Regression:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Class 0 (neg)       1.00      0.34      0.51      7038\n",
      "Class 1 (pos)       0.04      0.95      0.08       200\n",
      "\n",
      "     accuracy                           0.36      7238\n",
      "    macro avg       0.52      0.64      0.29      7238\n",
      " weighted avg       0.97      0.36      0.49      7238\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "--- Training Model: Random Forest ---\n",
      "Random Forest finished in 24.75 seconds.\n",
      "COST for Random Forest: $19,210\n",
      "\n",
      "Classification Report for Random Forest:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Class 0 (neg)       0.99      0.99      0.99      7038\n",
      "Class 1 (pos)       0.70      0.81      0.75       200\n",
      "\n",
      "     accuracy                           0.99      7238\n",
      "    macro avg       0.85      0.90      0.87      7238\n",
      " weighted avg       0.99      0.99      0.99      7238\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "--- Training Model: XGBoost ---\n",
      "XGBoost finished in 5.96 seconds.\n",
      "COST for XGBoost: $18,940\n",
      "\n",
      "Classification Report for XGBoost:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Class 0 (neg)       0.99      0.99      0.99      7038\n",
      "Class 1 (pos)       0.79      0.81      0.80       200\n",
      "\n",
      "     accuracy                           0.99      7238\n",
      "    macro avg       0.89      0.90      0.90      7238\n",
      " weighted avg       0.99      0.99      0.99      7238\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "--- Training Model: CatBoost ---\n",
      "CatBoost finished in 77.59 seconds.\n",
      "COST for CatBoost: $17,020\n",
      "\n",
      "Classification Report for CatBoost:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Class 0 (neg)       1.00      0.99      0.99      7038\n",
      "Class 1 (pos)       0.76      0.83      0.80       200\n",
      "\n",
      "     accuracy                           0.99      7238\n",
      "    macro avg       0.88      0.91      0.90      7238\n",
      " weighted avg       0.99      0.99      0.99      7238\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "--- Final Project Bake-Off Results ---\n",
      "\n",
      "--- Final Model Ranking (Lowest Cost is Best) ---\n",
      "1. CatBoost: $17,020\n",
      "2. XGBoost: $18,940\n",
      "3. Random Forest: $19,210\n",
      "4. Logistic Regression: $51,520\n",
      "\n",
      "--- OVERALL PROJECT CHAMPION ---\n",
      "Model: CatBoost\n",
      "Final Cost: $17,020\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# --- 8. Run the Phase 2 \"Model Bake-Off\" ---\n",
    "print(\"--- Phase 2: The Model Bake-Off ---\")\n",
    "print(\"We are testing 4 models on our single, processed dataset.\")\n",
    "\n",
    "# --- 8.1. Define the Contender Models ---\n",
    "models_to_test = {\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=42, max_iter=1000),\n",
    "    \n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "    \n",
    "    \"XGBoost\": XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "    \n",
    "    \"CatBoost\": CatBoostClassifier(random_state=42, verbose=False)\n",
    "}\n",
    "\n",
    "# --- 8.2. Run the Experiment ---\n",
    "phase_2_results = {}\n",
    "\n",
    "# We use our 'resampled' data from Cell 7\n",
    "# (X_train_resampled, y_train_resampled, X_test_processed, y_test)\n",
    "if 'X_train_resampled' not in locals():\n",
    "    print(\"ERROR: Training data not found. Please re-run Cell 7.\")\n",
    "    raise NameError(\"Missing data from Cell 7\")\n",
    "\n",
    "# We must use the non-reset y_test from Cell 3, as the processed\n",
    "# X_test_processed still matches its index.\n",
    "# If you used y_test_reset in Cell 6, use that. Otherwise, use y_test.\n",
    "# We will use 'y_test' from Cell 3 as it's the safest assumption.\n",
    "y_test_for_eval = y_test.astype(int)\n",
    "\n",
    "\n",
    "for model_name, model in models_to_test.items():\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "    print(f\"--- Training Model: {model_name} ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # 1. Train the model on the resampled data\n",
    "        model.fit(X_train_resampled, y_train_resampled)\n",
    "        \n",
    "        # 2. Predict on the processed test data\n",
    "        y_pred = model.predict(X_test_processed)\n",
    "        \n",
    "        # 3. Calculate cost\n",
    "        cost = total_cost(y_test_for_eval, y_pred)\n",
    "        phase_2_results[model_name] = cost\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print(f\"{model_name} finished in {end_time - start_time:.2f} seconds.\")\n",
    "        print(f\"COST for {model_name}: ${cost:,.0f}\")\n",
    "        \n",
    "        print(f\"\\nClassification Report for {model_name}:\")\n",
    "        print(classification_report(y_test_for_eval, y_pred, target_names=['Class 0 (neg)', 'Class 1 (pos)']))\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error training {model_name}: {e}\")\n",
    "        phase_2_results[model_name] = float('inf')\n",
    "\n",
    "# --- 8.3. Final Project Verdict ---\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "print(\"--- Final Project Bake-Off Results ---\")\n",
    "\n",
    "# Sort the results by cost\n",
    "sorted_model_results = sorted(phase_2_results.items(), key=lambda item: item[1])\n",
    "\n",
    "print(\"\\n--- Final Model Ranking (Lowest Cost is Best) ---\")\n",
    "for i, (model_name, cost) in enumerate(sorted_model_results):\n",
    "    print(f\"{i+1}. {model_name}: ${cost:,.0f}\")\n",
    "\n",
    "# Find the new winner\n",
    "overall_champion_name = sorted_model_results[0][0]\n",
    "overall_champion_cost = sorted_model_results[0][1]\n",
    "\n",
    "print(f\"\\n--- OVERALL PROJECT CHAMPION ---\")\n",
    "print(f\"Model: {overall_champion_name}\")\n",
    "print(f\"Final Cost: ${overall_champion_cost:,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13175839",
   "metadata": {},
   "source": [
    "# Phase 2: The \"Model Bake-Off\" Final Verdict\n",
    "\n",
    "We have successfully completed our \"Model Bake-Off.\" We tested our 4 contender models on the identical dataset prepared by our \"Practical Winner\" pipeline from Phase 1 (`Impute(0) -> RobustScaler -> SMOTE`).\n",
    "\n",
    "This \"apples-to-apples\" comparison gives us our definitive project champion.\n",
    "\n",
    "## Final Model Ranking\n",
    "\n",
    "| Rank | Model | Final Cost | Training Time |\n",
    "|---:|:---|---:|---:|\n",
    "| **1.** | **`CatBoostClassifier`** | **$17,020** | `77.59 sec` |\n",
    "| 2. | `XGBClassifier` | $18,940 | `5.96 sec` |\n",
    "| 3. | `RandomForestClassifier` | $19,210 | `24.75 sec` |\n",
    "| 4. | `LogisticRegression` | $51,520 | `44.55 sec` |\n",
    "\n",
    "---\n",
    "\n",
    "## In-Depth Analysis: The \"Why\"\n",
    "\n",
    "The results are incredibly clear. The entire competition comes down to the model's trade-off between **False Negatives (FN)** (missed trucks @ $500) and **False Positives (FP)** (false alarms @ $10).\n",
    "\n",
    "Based on the classification reports, we can reverse-engineer the exact error counts:\n",
    "\n",
    "### Cost Breakdown Comparison\n",
    "\n",
    "| Model | False Negatives (FN) | False Positives (FP) | FN Cost (FN * $500) | FP Cost (FP * $10) | Total Cost |\n",
    "|:---|---:|---:|---:|---:|---:|\n",
    "| **CatBoost (Winner)** | **33** | 52 | **$16,500** | $520 | **$17,020** |\n",
    "| XGBoost (Runner-Up) | 37 | 44 | $18,500 | $440 | $18,940 |\n",
    "| RandomForest | 37 | 71 | $18,500 | $710 | $19,210 |\n",
    "| LogisticRegression | 10 | 4,652 | $5,000 | $46,520 | $51,520 |\n",
    "\n",
    "### Key Strategic Insights\n",
    "\n",
    "#### 1. The CatBoost vs. XGBoost Showdown\n",
    "\n",
    "This is the core story of our project.\n",
    "\n",
    "* `CatBoost` saved **4 more trucks** than `XGBoost` (it had 33 False Negatives vs. 37). This saved us **$2,000** in breakdown costs.\n",
    "* To achieve this, it was slightly more aggressive, creating 8 more false alarms (52 vs. 44), which cost an extra $80.\n",
    "* The final trade-off is a **$1,920 net victory for `CatBoost`** ($2,000 saved - $80 cost). This is a massive, unambiguous win and proves `CatBoost` is the superior model for this problem.\n",
    "\n",
    "#### 2. Why the Others Failed\n",
    "\n",
    "* **`RandomForest`** was no better at catching failures than `XGBoost` (both had 37 FNs) but was much \"noisier\" (71 FPs), making it more expensive.\n",
    "* **`LogisticRegression`** completely failed, proving our problem is complex and not linearly separable. It created over 4,600 false alarms, making it unusable despite its high recall.\n",
    "\n",
    "---\n",
    "\n",
    "# Overall Project Conclusion: The Champion Strategy\n",
    "\n",
    "Our entire sandbox experiment, from challenging the reference notebook to our final model bake-off, has been a complete success. We have scientifically proven a final, champion strategy for your production pipeline based on the raw, difficult dataset.\n",
    "\n",
    "* **Preprocessing: `SimpleImputer(strategy='constant', fill_value=0)` + `RobustScaler()`**\n",
    "    * We proved this fast, simple strategy was our \"Practical Winner\" in Phase 1, beating more complex methods.\n",
    "\n",
    "* **Imbalance Handling: `SMOTE()`**\n",
    "    * We proved this was definitively better than `scale_pos_weight`, saving an additional $880.\n",
    "\n",
    "* **Model: `CatBoostClassifier`**\n",
    "    * We proved it is the best model for this data, saving an additional $1,920 over the next-best competitor.\n",
    "\n",
    "This final combination (**Impute(0) -> Scale -> SMOTE -> CatBoost**) is the one that gives the lowest-cost solution of **$17,020**, and every single step is now backed by a data-driven, cost-based experiment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
