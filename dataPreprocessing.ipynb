{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7df2010a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- All libraries imported successfully. ---\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# --- Preprocessing ---\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler, PowerTransformer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# --- Imbalance Handling ---\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline # Special pipeline for samplers\n",
    "\n",
    "# --- Modeling ---\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import miceforest as mf\n",
    "\n",
    "# --- Evaluation ---\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "# --- Settings ---\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(\"--- All libraries imported successfully. ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c407c65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- `reduce_memory_usage` function defined. ---\n"
     ]
    }
   ],
   "source": [
    "# --- Function : Upgraded Memory Reducer from EDA---\n",
    "\n",
    "def reduce_memory_usage(df):\n",
    "    \"\"\"\n",
    "    Iterates through all columns of a DataFrame and modifies the data type\n",
    "    to reduce memory usage.\n",
    "    \n",
    "    - Checks if float columns contain only whole numbers.\n",
    "    - If they do, converts them to a memory-efficient nullable integer type\n",
    "      (e.g., pd.Int32Dtype()) to preserve NaNs.\n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    print(f'\\nInitial memory usage of the dataframe: {start_mem:.2f} MB')\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        # Skip the categorical 'class' column\n",
    "        if col == 'class':\n",
    "            continue\n",
    "            \n",
    "        # Downcast numerical columns\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "\n",
    "            # Check for integers\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            \n",
    "            # --- START: New Logic for Float Columns ---\n",
    "            else: # This is a float column\n",
    "                \n",
    "                # Check if all non-NaN values are whole numbers\n",
    "                if df[col].dropna().apply(lambda x: x.is_integer()).all():\n",
    "                    print(f\"Column '{col}' is float but contains only whole numbers. Converting to nullable int.\")\n",
    "                    \n",
    "                    # We can now use Pandas' Nullable Integer types\n",
    "                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                        df[col] = df[col].astype(pd.Int8Dtype())\n",
    "                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                        df[col] = df[col].astype(pd.Int16Dtype())\n",
    "                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                        df[col] = df[col].astype(pd.Int32Dtype())\n",
    "                    else:\n",
    "                        df[col] = df[col].astype(pd.Int64Dtype())\n",
    "                \n",
    "                else:\n",
    "                    # --- This is a \"real\" float column (has decimals) ---\n",
    "                    if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                        df[col] = df[col].astype(np.float32)\n",
    "                    else:\n",
    "                        df[col] = df[col].astype(np.float64)\n",
    "            # --- END: New Logic ---\n",
    "\n",
    "    end_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    print(f'Memory usage after optimization: {end_mem:.2f} MB')\n",
    "    print(f'Reduced by: {(100 * (start_mem - end_mem) / start_mem):.2f}%')\n",
    "    return df\n",
    "\n",
    "print(\"--- `reduce_memory_usage` function defined. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8918e922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- `total_cost` function defined. ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Function : Our Business Cost Function ---\n",
    "\n",
    "def total_cost(y_true, y_pred):\n",
    "    '''\n",
    "    Calculates the total cost based on the problem statement.\n",
    "    Cost 1 (False Positive) = 10\n",
    "    Cost 2 (False Negative) = 500\n",
    "    '''\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    try:\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        \n",
    "        # Handle cases where the model predicts only one class\n",
    "        if cm.shape == (1, 1):\n",
    "            if y_true.unique()[0] == 0: # Only negative class\n",
    "                tn = cm[0, 0]\n",
    "                fp, fn, tp = 0, 0, 0\n",
    "            else: # Only positive class\n",
    "                tp = cm[0, 0]\n",
    "                tn, fp, fn = 0, 0, 0\n",
    "        else:\n",
    "             # Standard 2x2 matrix\n",
    "            tn, fp, fn, tp = cm.ravel()\n",
    "            \n",
    "    except Exception as e:\n",
    "        # Fallback for complex edge cases\n",
    "        if len(np.unique(y_pred)) == 1:\n",
    "            if np.unique(y_pred)[0] == 0: # Predicted all negatives\n",
    "                tn = np.sum((y_true == 0))\n",
    "                fn = np.sum((y_true == 1))\n",
    "                fp, tp = 0, 0\n",
    "            else: # Predicted all positives\n",
    "                fp = np.sum((y_true == 0))\n",
    "                tp = np.sum((y_true == 1))\n",
    "                tn, fn = 0, 0\n",
    "        else:\n",
    "            print(f\"Error in confusion matrix calculation: {e}. Returning high cost.\")\n",
    "            return np.inf\n",
    "            \n",
    "    cost = (10 * fp) + (500 * fn)\n",
    "    return cost\n",
    "\n",
    "print(\"--- `total_cost` function defined. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00fc332b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Successfully loaded data from URL. Initial shape: (36188, 171) ---\n",
      "\n",
      "Initial memory usage of the dataframe: 48.73 MB\n",
      "Column 'ab_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ac_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ad_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ae_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'af_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ag_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ag_001' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ag_002' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ag_003' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ag_004' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ag_005' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ag_006' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ag_007' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ag_008' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ag_009' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ah_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ai_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'aj_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ak_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'al_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'am_0' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'an_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ao_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ap_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'aq_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ar_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'as_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'at_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'au_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'av_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ax_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ay_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ay_001' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ay_002' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ay_003' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ay_004' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ay_005' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ay_006' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ay_007' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ay_008' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ay_009' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'az_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'az_001' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'az_002' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'az_003' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'az_004' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'az_005' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'az_006' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'az_007' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'az_008' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'az_009' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ba_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ba_001' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ba_002' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ba_003' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ba_004' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ba_005' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ba_006' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ba_007' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ba_008' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ba_009' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'bb_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'bc_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'bd_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'be_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'bf_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'bg_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'bh_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'bi_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'bj_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'bk_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'bl_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'bm_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'bn_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'bo_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'bp_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'bq_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'br_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'bs_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'bu_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'bv_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'bx_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'by_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'bz_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ca_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cb_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cc_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cd_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ce_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cf_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cg_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ch_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cl_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cm_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cn_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cn_001' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cn_002' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cn_003' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cn_004' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cn_005' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cn_006' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cn_007' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cn_008' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cn_009' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'co_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cp_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cq_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cr_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cs_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cs_001' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cs_002' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cs_003' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cs_004' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cs_005' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cs_006' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cs_007' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cs_008' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cs_009' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ct_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cu_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cv_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cx_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cy_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'cz_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'da_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'db_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'dc_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'dd_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'de_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'df_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'dg_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'dh_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'di_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'dj_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'dk_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'dl_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'dm_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'dn_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'do_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'dp_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'dq_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'dr_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ds_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'dt_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'du_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'dv_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'dx_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'dy_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'dz_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ea_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'eb_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ed_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ee_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ee_001' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ee_002' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ee_003' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ee_004' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ee_005' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ee_006' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ee_007' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ee_008' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ee_009' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'ef_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Column 'eg_000' is float but contains only whole numbers. Converting to nullable int.\n",
      "Memory usage after optimization: 30.58 MB\n",
      "Reduced by: 37.25%\n",
      "\n",
      "--- Encoding target variable 'class' ---\n",
      "Initial class counts:\n",
      "class\n",
      "neg    35188\n",
      "pos     1000\n",
      "Name: count, dtype: int64\n",
      "Encoded class counts:\n",
      "class\n",
      "0    35188\n",
      "1     1000\n",
      "Name: count, dtype: Int64\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Load Data ---\n",
    "file_path = \"https://raw.githubusercontent.com/avnyadav/sensor-fault-detection/main/aps_failure_training_set1.csv\"\n",
    "try:\n",
    "    df = pd.read_csv(file_path, na_values='na')\n",
    "    print(f\"--- Successfully loaded data from URL. Initial shape: {df.shape} ---\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    # Stop execution if data loading fails\n",
    "    raise\n",
    "\n",
    "# --- 2. Reduce Memory ---\n",
    "# This will use the upgraded function from Cell 1\n",
    "df = reduce_memory_usage(df)\n",
    "\n",
    "# --- 3. Encode Target Variable ---\n",
    "print(\"\\n--- Encoding target variable 'class' ---\")\n",
    "# Check if 'class' column exists\n",
    "if 'class' not in df.columns:\n",
    "    print(\"Error: 'class' column not found.\")\n",
    "else:\n",
    "    initial_class_counts = df['class'].value_counts()\n",
    "    print(f\"Initial class counts:\\n{initial_class_counts}\")\n",
    "    \n",
    "    # Map 'neg' to 0 and 'pos' to 1\n",
    "    df['class'] = df['class'].map({'neg': 0, 'pos': 1})\n",
    "    \n",
    "    # Verify mapping\n",
    "    if df['class'].isnull().any():\n",
    "        print(\"Warning: Found NaN values in 'class' column after mapping. Check for unexpected values.\")\n",
    "    else:\n",
    "        # Use a nullable int type here as well, just for consistency\n",
    "        df['class'] = df['class'].astype(pd.Int8Dtype())\n",
    "        print(f\"Encoded class counts:\\n{df['class'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b9a1bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting initial feature cleanup ---\n",
      "Initial number of features: 170\n",
      "Dropped 7 columns with > 70% missing values.\n",
      "Columns dropped: ['ab_000', 'bn_000', 'bo_000', 'bp_000', 'bq_000', 'br_000', 'cr_000']\n",
      "Dropped 1 columns with constant values (zero variance).\n",
      "Columns dropped: ['cd_000']\n",
      "\n",
      "--- Cleanup complete ---\n",
      "Final shape of X: (36188, 162)\n",
      "Final shape of y: (36188,)\n",
      "Total numeric features remaining: 162\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 4. Initial Feature Cleanup ---\n",
    "print(\"\\n--- Starting initial feature cleanup ---\")\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "if 'class' in df.columns:\n",
    "    X = df.drop('class', axis=1)\n",
    "    y = df['class']\n",
    "else:\n",
    "    raise ValueError(\"Cannot proceed without 'class' column for X/y split.\")\n",
    "\n",
    "print(f\"Initial number of features: {X.shape[1]}\")\n",
    "\n",
    "# --- 4a. Drop High-NaN Columns (> 70% missing) ---\n",
    "missing_percentage = (X.isnull().sum() / len(X)) * 100\n",
    "cols_to_drop_nan = missing_percentage[missing_percentage > 70].index\n",
    "\n",
    "if len(cols_to_drop_nan) > 0:\n",
    "    X = X.drop(columns=cols_to_drop_nan)\n",
    "    print(f\"Dropped {len(cols_to_drop_nan)} columns with > 70% missing values.\")\n",
    "    print(f\"Columns dropped: {list(cols_to_drop_nan)}\")\n",
    "else:\n",
    "    print(\"No columns had > 70% missing values.\")\n",
    "\n",
    "# --- 4b. Drop Constant Columns (Zero Variance) ---\n",
    "# We check for columns that have only 1 unique value (or 0 unique values if all NaN)\n",
    "unique_counts = X.nunique()\n",
    "cols_to_drop_constant = unique_counts[unique_counts <= 1].index\n",
    "\n",
    "if len(cols_to_drop_constant) > 0:\n",
    "    X = X.drop(columns=cols_to_drop_constant)\n",
    "    print(f\"Dropped {len(cols_to_drop_constant)} columns with constant values (zero variance).\")\n",
    "    print(f\"Columns dropped: {list(cols_to_drop_constant)}\")\n",
    "else:\n",
    "    print(\"No constant columns found.\")\n",
    "\n",
    "# --- 5. Final Check ---\n",
    "print(f\"\\n--- Cleanup complete ---\")\n",
    "print(f\"Final shape of X: {X.shape}\")\n",
    "print(f\"Final shape of y: {y.shape}\")\n",
    "\n",
    "# Store all remaining feature names for the ColumnTransformer\n",
    "# All remaining features are numeric\n",
    "numeric_features = list(X.columns)\n",
    "print(f\"Total numeric features remaining: {len(numeric_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7c997b",
   "metadata": {},
   "source": [
    "# Analysis of Cell 2 Achievements\n",
    "\n",
    "## What We Achieved\n",
    "\n",
    "* **Data Loaded:** We successfully loaded the full dataset from the URL, which started with **36,188 rows** and **171 columns** (170 features + 1 target).\n",
    "\n",
    "* **Memory Optimized:** The `reduce_memory_usage` function worked perfectly. The log message *Column like 'ab000, ae000 , ad000' is float but contains only whole numbers. Converting them to *nullable *int correct correctly.\n",
    "    * This stores \"integer-like\" data more efficiently while correctly preserving `NaN` values.\n",
    "    * Memory usage was reduced from 48.73 MB to 30.58 MB (a **37.25% reduction**).\n",
    "\n",
    "* **Target Encoded:** We've successfully converted the `class` column from 'neg'/'pos' to binary **0/1**.\n",
    "\n",
    "* **Initial Pruning:** We \"pruned\" our feature set `X` by removing 8 problematic columns:\n",
    "    * **High-NaN Columns ( $>70\\%$):** Dropped 7 columns (`['ab_000', 'bn_000', 'bo_000', 'bp_000', 'bq_000', 'br_000', 'cr_000']`). These are considered noise, not signal.\n",
    "    * **Constant Column (Zero Variance):** Dropped 1 column (`['cd_000']`). This column had no predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67db069d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Splitting data into Train and Test sets ---\n",
      "\n",
      "--- Verification of stratified split: ---\n",
      "X_train shape: (28950, 162)\n",
      "X_test shape: (7238, 162)\n",
      "\n",
      "--- Training Set Class Distribution ---\n",
      "class\n",
      "0    28150\n",
      "1      800\n",
      "Name: count, dtype: Int64\n",
      "Positive class percentage in train: 2.76%\n",
      "\n",
      "--- Test Set Class Distribution ---\n",
      "class\n",
      "0    7038\n",
      "1     200\n",
      "Name: count, dtype: Int64\n",
      "Positive class percentage in test: 2.76%\n"
     ]
    }
   ],
   "source": [
    "# --- 6. Create Train-Test Split (The \"Data Leakage Barrier\") ---\n",
    "print(\"\\n--- Splitting data into Train and Test sets ---\")\n",
    "\n",
    "# We use stratify=y to ensure the rare positive class is\n",
    "# represented in both train and test sets proportionally.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# --- 7. Verify the Split ---\n",
    "print(\"\\n--- Verification of stratified split: ---\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "\n",
    "print(\"\\n--- Training Set Class Distribution ---\")\n",
    "train_counts = y_train.value_counts()\n",
    "print(train_counts)\n",
    "print(f\"Positive class percentage in train: {(train_counts[1] / len(y_train) * 100):.2f}%\")\n",
    "\n",
    "print(\"\\n--- Test Set Class Distribution ---\")\n",
    "test_counts = y_test.value_counts()\n",
    "print(test_counts)\n",
    "print(f\"Positive class percentage in test: {(test_counts[1] / len(y_test) * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b1afb4",
   "metadata": {},
   "source": [
    "## Stratification:\n",
    "\n",
    "* Training Set: Has **800 'pos'** samples (80% of the total 1000).\n",
    "\n",
    "* Test Set: Has **200 'pos'** samples (20% of the total 1000).\n",
    "\n",
    "* The **2.76% positive class ratio** is identical in both sets, meaning our test set is a perfect representation of our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64299e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Defining Preprocessing & Modeling Pipelines ---\n",
      "Calculated 'scale_pos_weight' for XGBoost: 35.19\n",
      "\n",
      "--- Successfully defined 3 complete pipelines: ---\n",
      "1. pipeline_1 (Reference: Impute 0 -> Scale)\n",
      "2. pipeline_2 (EDA-Median: Impute Median -> Transform -> Scale)\n",
      "3. pipeline_3 (EDA-KNN: Transform -> Impute KNN -> Scale)\n",
      "\n",
      "All pipelines end with XGBClassifier using scale_pos_weight.\n"
     ]
    }
   ],
   "source": [
    "# --- 8. Define Phase 1 Pipelines (Pipelines 1, 2, 3) ---\n",
    "print(\"--- Defining Preprocessing & Modeling Pipelines ---\")\n",
    "\n",
    "# --- Define the Model we will use as our \"Judge\" ---\n",
    "# We use scale_pos_weight for imbalance, as it's often faster\n",
    "# and a good alternative to SMOTE. Let's calculate it.\n",
    "# scale_pos_weight = total_negatives / total_positives\n",
    "train_counts = y_train.value_counts()\n",
    "scale_pos_weight_value = train_counts[0] / train_counts[1]\n",
    "print(f\"Calculated 'scale_pos_weight' for XGBoost: {scale_pos_weight_value:.2f}\")\n",
    "\n",
    "# We will use this as our constant \"judge\" model\n",
    "# We set eval_metric to 'logloss' to avoid a common warning\n",
    "constant_model = XGBClassifier(\n",
    "    scale_pos_weight=scale_pos_weight_value,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "# === Pipeline 1: The \"Reference\" (Control Group) ===\n",
    "# Hypothesis: Filling NaNs with 0 is the best strategy.\n",
    "# Steps: 1. Impute(0) -> 2. Scale\n",
    "preprocessor_1_ref = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value=0)),\n",
    "            ('scaler', RobustScaler())])\n",
    "        , numeric_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# === Pipeline 2: \"EDA-Informed\" (Median + Skew Correction) ===\n",
    "# Hypothesis: Fixing skew is more important.\n",
    "# Steps: 1. Impute(Median) -> 2. Transform(Yeo-Johnson) -> 3. Scale\n",
    "preprocessor_2_eda = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('transformer', PowerTransformer(method='yeo-johnson')),\n",
    "            ('scaler', RobustScaler()) ])\n",
    "        , numeric_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# === Pipeline 3: \"EDA-Informed\" (KNN + Skew Correction) ===\n",
    "# Hypothesis: Smart imputer (KNN) after fixing skew is best.\n",
    "# Steps: 1. Transform(Yeo-Johnson) -> 2. Impute(KNN) -> 3. Scale\n",
    "preprocessor_3_knn = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline(steps=[\n",
    "            ('transformer', PowerTransformer(method='yeo-johnson')),\n",
    "            ('imputer', KNNImputer(n_neighbors=5)),\n",
    "            ('scaler', RobustScaler())\n",
    "        ]), numeric_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# --- Now, create the *full* pipelines (Prep + Model) ---\n",
    "# NOTE: We are using XGBoost's built-in `scale_pos_weight`\n",
    "# which is simpler and faster than adding a SMOTE step.\n",
    "# This is a common and robust way to handle imbalance.\n",
    "\n",
    "pipeline_1 = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_1_ref),\n",
    "    ('model', constant_model)\n",
    "])\n",
    "\n",
    "pipeline_2 = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_2_eda),\n",
    "    ('model', constant_model)\n",
    "])\n",
    "\n",
    "pipeline_3 = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_3_knn),\n",
    "    ('model', constant_model)\n",
    "])\n",
    "\n",
    "print(\"\\n--- Successfully defined 3 complete pipelines: ---\")\n",
    "print(\"1. pipeline_1 (Reference: Impute 0 -> Scale)\")\n",
    "print(\"2. pipeline_2 (EDA-Median: Impute Median -> Transform -> Scale)\")\n",
    "print(\"3. pipeline_3 (EDA-KNN: Transform -> Impute KNN -> Scale)\")\n",
    "print(\"\\nAll pipelines end with XGBClassifier using scale_pos_weight.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa73c1b4",
   "metadata": {},
   "source": [
    "# Analysis of Cell 9 Output\n",
    "\n",
    "## What We Achieved\n",
    "\n",
    "* **Pipelines Built:** We have successfully defined our first three experimental pipelines, encapsulating our competing hypotheses.\n",
    "    * **Hypothesis 1 (Reference):** `pipeline_1` is ready to test if `Impute(0) -> Scale` is the best path.\n",
    "    * **Hypothesis 2 (EDA-Median):** `pipeline_2` is ready to test if `Impute(Median) -> Transform -> Scale` is better.\n",
    "    * **Hypothesis 3 (EDA-KNN):** `pipeline_3` is ready to test if `Transform -> Impute(KNN) -> Scale` is the best.\n",
    "\n",
    "* **Imbalance Handled:** We've made a strategic choice to use XGBoost's built-in `scale_pos_weight` parameter (calculated at $35.19$). This is a powerful and computationally efficient way to force the model to pay $35.19\\text{x}$ more attention to the rare positive class, directly addressing our core problem (instead of using SMOTE).\n",
    "\n",
    "---\n",
    "\n",
    "## Key Findings\n",
    "\n",
    "* All our \"test tubes\" are labeled and sitting in the rack. We are now ready to run the actual experiment by fitting these pipelines to the training data and using them to predict on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73d7c794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Pipeline 1 (Reference: Impute 0 -> Scale) ---\n",
      "Pipeline 1 finished in 9.02 seconds.\n",
      "COST for Pipeline 1: $19,820\n",
      "\n",
      "Classification Report for Pipeline 1:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Class 0 (neg)       0.99      1.00      0.99      7038\n",
      "Class 1 (pos)       0.83      0.81      0.82       200\n",
      "\n",
      "     accuracy                           0.99      7238\n",
      "    macro avg       0.91      0.90      0.91      7238\n",
      " weighted avg       0.99      0.99      0.99      7238\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "--- Training Pipeline 2 (EDA-Median: Impute Median -> Transform -> Scale) ---\n",
      "Pipeline 2 finished in 14.10 seconds.\n",
      "COST for Pipeline 2: $20,350\n",
      "\n",
      "Classification Report for Pipeline 2:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Class 0 (neg)       0.99      1.00      0.99      7038\n",
      "Class 1 (pos)       0.82      0.80      0.81       200\n",
      "\n",
      "     accuracy                           0.99      7238\n",
      "    macro avg       0.91      0.90      0.90      7238\n",
      " weighted avg       0.99      0.99      0.99      7238\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "--- Training Pipeline 3 (EDA-KNN: Transform -> Impute KNN -> Scale) ---\n",
      "NOTE: This pipeline will take the longest due to KNN Imputer.\n",
      "Pipeline 3 finished in 785.96 seconds.\n",
      "COST for Pipeline 3: $19,890\n",
      "\n",
      "Classification Report for Pipeline 3:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Class 0 (neg)       0.99      0.99      0.99      7038\n",
      "Class 1 (pos)       0.81      0.81      0.81       200\n",
      "\n",
      "     accuracy                           0.99      7238\n",
      "    macro avg       0.90      0.90      0.90      7238\n",
      " weighted avg       0.99      0.99      0.99      7238\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "--- Comparison of Pipeline 1, 2, and 3 ---\n",
      "{'Pipeline 1 (Reference)': np.int64(19820), 'Pipeline 2 (EDA-Median)': np.int64(20350), 'Pipeline 3 (EDA-KNN)': np.int64(19890)}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# We will store the results of our experiments in this dictionary\n",
    "phase_1_results = {}\n",
    "\n",
    "# --- 1. Fit and Evaluate Pipeline 1 (Reference) ---\n",
    "print(\"--- Training Pipeline 1 (Reference: Impute 0 -> Scale) ---\")\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Train the pipeline\n",
    "    pipeline_1.fit(X_train, y_train)\n",
    "    \n",
    "    # Get predictions on the test set\n",
    "    y_pred_1 = pipeline_1.predict(X_test)\n",
    "    \n",
    "    # Calculate the total cost\n",
    "    cost_1 = total_cost(y_test, y_pred_1)\n",
    "    \n",
    "    # Store the result\n",
    "    phase_1_results['Pipeline 1 (Reference)'] = cost_1\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Pipeline 1 finished in {end_time - start_time:.2f} seconds.\")\n",
    "    print(f\"COST for Pipeline 1: ${cost_1:,.0f}\")\n",
    "    \n",
    "    print(\"\\nClassification Report for Pipeline 1:\")\n",
    "    # We must convert y_test (nullable int) to a standard int for the report\n",
    "    print(classification_report(y_test.astype(int), y_pred_1, target_names=['Class 0 (neg)', 'Class 1 (pos)']))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error training Pipeline 1: {e}\")\n",
    "    phase_1_results['Pipeline 1 (Reference)'] = float('inf') # Assign high cost on failure\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# --- 2. Fit and Evaluate Pipeline 2 (EDA-Median) ---\n",
    "print(\"--- Training Pipeline 2 (EDA-Median: Impute Median -> Transform -> Scale) ---\")\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Train the pipeline\n",
    "    pipeline_2.fit(X_train, y_train)\n",
    "    \n",
    "    # Get predictions on the test set\n",
    "    y_pred_2 = pipeline_2.predict(X_test)\n",
    "    \n",
    "    # Calculate the total cost\n",
    "    cost_2 = total_cost(y_test, y_pred_2)\n",
    "    \n",
    "    # Store the result\n",
    "    phase_1_results['Pipeline 2 (EDA-Median)'] = cost_2\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Pipeline 2 finished in {end_time - start_time:.2f} seconds.\")\n",
    "    print(f\"COST for Pipeline 2: ${cost_2:,.0f}\")\n",
    "    \n",
    "    print(\"\\nClassification Report for Pipeline 2:\")\n",
    "    print(classification_report(y_test.astype(int), y_pred_2, target_names=['Class 0 (neg)', 'Class 1 (pos)']))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error training Pipeline 2: {e}\")\n",
    "    phase_1_results['Pipeline 2 (EDA-Median)'] = float('inf')\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# --- 3. Fit and Evaluate Pipeline 3 (EDA-KNN) ---\n",
    "print(\"--- Training Pipeline 3 (EDA-KNN: Transform -> Impute KNN -> Scale) ---\")\n",
    "print(\"NOTE: This pipeline will take the longest due to KNN Imputer.\")\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Train the pipeline\n",
    "    pipeline_3.fit(X_train, y_train)\n",
    "    \n",
    "    # Get predictions on the test set\n",
    "    y_pred_3 = pipeline_3.predict(X_test)\n",
    "    \n",
    "    # Calculate the total cost\n",
    "    cost_3 = total_cost(y_test, y_pred_3)\n",
    "    \n",
    "    # Store the result\n",
    "    phase_1_results['Pipeline 3 (EDA-KNN)'] = cost_3\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Pipeline 3 finished in {end_time - start_time:.2f} seconds.\")\n",
    "    print(f\"COST for Pipeline 3: ${cost_3:,.0f}\")\n",
    "    \n",
    "    print(\"\\nClassification Report for Pipeline 3:\")\n",
    "    print(classification_report(y_test.astype(int), y_pred_3, target_names=['Class 0 (neg)', 'Class 1 (pos)']))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error training Pipeline 3: {e}\")\n",
    "    phase_1_results['Pipeline 3 (EDA-KNN)'] = float('inf')\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "print(\"--- Comparison of Pipeline 1, 2, and 3 ---\")\n",
    "print(phase_1_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3651ed41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
